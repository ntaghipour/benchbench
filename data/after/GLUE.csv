Model,Rank,Rank-Before,Average,Average-Before,CoLA (× 0.0060),SST-2 (× 0.0314),MRPC (× 0.0211),STS-B (× 0.0117),QQP (× 1.0000),QNLI (× 0.0206),RTE (× 0.0060),WNLI (× 0.0047),MNLI (× 0.0161)
Vega v1,1.0,2.0,10.551224245678121,91.27222222222223,0.44208602967699817,3.0714106483975416,1.9770596348138296,1.0883415445695344,83.9,1.9925118504142614,0.5523390913992999,0.4569908947651181,1.4802785170665151
Turing NLR v5 ,2.0,3.0,10.534826806967697,91.17777777777778,0.4348976389505429,3.0619987669417776,1.9601526577122683,1.0906745382342065,83.75,2.017237954038844,0.5625011742497198,0.4476550235748195,1.488323509007094
DeBERTa + CLEVER,3.0,4.0,10.530643687953875,91.10555555555555,0.4474773227218396,3.0619987669417776,1.9485291109549445,1.0877582961533665,83.75,1.9925118504142614,0.5571212480347917,0.450922578491424,1.479474017872457
Turing ULR v6,4.0,1.0,10.522348220747839,91.27777777777777,0.4390908668743085,3.05886147312319,1.970719518400744,1.0883415445695344,83.65,1.9925118504142614,0.5595123263525374,0.4569908947651181,1.4851055122308625
ALBERT + DAAF + NAS,5.0,9.0,10.5062463043121,90.56666666666668,0.4402889319953843,3.0494495916674262,1.9654360880565063,1.081342563575518,83.55,2.008995919497316,0.5481547043432445,0.4411199137416104,1.4714290259318783
DeBERTa / TuringNLRv4,6.0,7.0,10.505894822569715,90.76666666666668,0.4283082807846256,3.05886147312319,1.9654360880565063,1.0819258119916861,83.5,2.044024566298808,0.5571212480347917,0.4411199137416104,1.4762560210962257
StructBERT + CLEVER,7.0,6.0,10.471774900371855,91.01111111111112,0.45107151808506724,3.065136060760366,1.9633227159188111,1.0883415445695344,83.19999999999999,2.0069354108619346,0.5529368609787363,0.444387468658215,1.473842523514052
ELECTRA-Large + Standard Tricks,8.0,15.0,10.455958230768424,89.44999999999999,0.42950634590570147,3.0463122978488384,1.9421889945418596,1.081342563575518,83.19999999999999,1.9739672726958246,0.5367970823339516,0.4285164876347073,1.4649930323794154
ERNIE,9.0,5.0,10.455740122385443,91.08888888888889,0.45226958320614313,3.0682733545789533,1.9622660298499632,1.0825090604078542,83.05000000000001,2.0048749022265526,0.5535346305581728,0.4476550235748195,1.4802785170665151
Funnel-Transformer (Ensemble B10-10-10H1024),10.0,14.0,10.442393229997297,89.70555555555556,0.4223179551792462,3.05886147312319,1.9506424830926403,1.0784263214946779,83.05000000000001,1.9739672726958246,0.5379926214928246,0.4411199137416104,1.468211029155647
2digit LANet,11.0,16.0,10.434216517551887,89.28333333333333,0.43010537846623936,3.052586885486014,1.923168645302603,1.083092308824022,83.0,1.9863303245081159,0.5445680868666257,0.41264550661119964,1.4754515219021678
NEZHA-Large,12.0,12.0,10.432025796135935,89.84999999999998,0.42950634590570147,3.052586885486014,1.9474724248860973,1.0749268309976698,82.95,1.9822093072373521,0.539785930231134,0.4411199137416104,1.4706245267378206
T5,13.0,10.0,10.425715826406414,90.30555555555556,0.4289073133451635,3.05886147312319,1.9358488781287737,1.084258805656358,82.85,1.9966328676850253,0.5547301697170457,0.4411199137416104,1.4810830162605733
ANNA,14.0,13.0,10.424439485953295,89.8111111111111,0.41153536908956334,3.0431750040302505,1.9316221338533837,1.0836755572401904,82.9,1.9780882899665884,0.548752473922681,0.4476550235748195,1.4754515219021678
MacALBERT + DKM,15.0,8.0,10.408145051098911,90.6888888888889,0.44807635528237755,3.0431750040302505,1.9770596348138296,1.081342563575518,82.65,2.0151774454034617,0.5499480130815539,0.4411199137416104,1.4674065299615888
ELECTRA-Large-NewSCL(single),16.0,24.0,10.407819910638477,85.56666666666665,0.4390908668743085,3.0494495916674262,1.9326788199222311,1.0714273405006616,82.94999999999999,1.9698462554250609,0.5194617645302939,0.28147651638750376,1.4569480404388364
 DropAttack-RoBERTa-large,17.0,17.0,10.404667137250307,88.77777777777777,0.4211198900581703,3.033763122574487,1.9305654477845358,1.0725938373329975,82.8,1.9636647295189154,0.5373948519133881,0.4187138228848937,1.4641885331853575
ELECTRA-large-M (bert4keras),18.0,20.0,10.395097306625793,88.27222222222221,0.4151295644527909,3.005527478207196,1.9210552731649078,1.0632618626743093,82.8,1.9327570999881873,0.5254394603246587,0.4285164876347073,1.4641885331853575
FreeLB-RoBERTa (ensemble),19.0,18.0,10.376787999192683,88.41111111111111,0.40734214116579776,3.0369004163930744,1.9432456806107066,1.0755100794138377,82.55,1.9698462554250609,0.5302216169601505,0.4154462679682892,1.4625795347972417
HIRE-RoBERTa,20.0,19.0,10.343278948642908,88.32777777777778,0.41093633652902534,3.0463122978488384,1.9411323084730117,1.075510079413838,82.25,1.967785746789679,0.5254394603246587,0.4154462679682892,1.4569480404388366
RoBERTa,21.0,21.0,10.339160075620704,88.10555555555555,0.40614407604472186,3.033763122574487,1.9242253313714504,1.073760334165334,82.25,1.9657252381542973,0.5272327690629681,0.4154462679682892,1.4561435412447785
MT-DNN-SMART,22.0,11.0,10.338721586604372,89.85555555555555,0.4163276295738668,3.05886147312319,1.9580392855745732,1.081342563575518,82.05000000000001,2.044024566298808,0.5361993127545152,0.4411199137416104,1.4625795347972417
MT-DNN-ensemble,23.0,22.0,10.283444969455763,87.56111111111109,0.40973827140794955,3.027488534937311,1.9337355059910786,1.0603456205934694,81.80000000000001,1.9780882899665884,0.5158751470536751,0.4154462679682892,1.410287087183479
ConvBERT-base-paddle-v1.1,24.0,29.0,10.266485933992065,83.10000000000001,0.3971585876366528,2.9929783029328445,1.9041482960633462,1.0451811617731006,81.95,1.922454556811278,0.4674558111193209,0.3038826072442205,1.4151140823478263
distilRoBERTa+GAL (6-layer transformer single model),25.0,33.0,10.229156322027409,82.65000000000002,0.35941953632276274,2.989841009114256,1.913658470682975,1.0475141554377727,81.65,1.910091504998987,0.4889755159790338,0.3038826072442205,1.3990240984666684
Snorkel MeTaL,26.0,27.0,10.219948377793955,83.18888888888888,0.38218277362320435,3.0180766534815473,1.902034923925651,1.0486806522701089,81.5,1.9348176086235693,0.48359558976410566,0.3038826072442205,1.4062645912131895
ConvBERT base,27.0,26.0,10.21791930727021,83.21666666666667,0.40614407604472186,3.002390184388608,1.8988648657191083,1.050430397518613,81.5,1.9203940481758963,0.4656625023810115,0.3038826072442205,1.4135050839597103
XLM (English only),28.0,28.0,10.215864448193946,83.1222222222222,0.3767914805783629,2.9992528905700198,1.8787878304110044,1.032349696617404,81.5,1.936878117258951,0.4543048803717185,0.3356245692912359,1.4287905686468103
RobustRoBERTa,29.0,39.0,10.210965814184538,81.9,0.3809847085021285,3.0369004163930744,1.9041482960633462,1.0492639006862767,81.45,1.9595437122481514,0.30067809845654525,0.37390164117146024,1.4432715541398524
SemBERT,30.0,30.0,10.190317820755538,82.92777777777776,0.3731972852151353,2.967879952384141,1.8967514935814132,1.0177684862132035,81.3,1.9492411690712421,0.5051152946238185,0.3038826072442205,1.3990240984666682
CombinedKD-TinyRoBERTa (6 layer 82M parameters  MATE-KD + AnnealingKD),31.0,42.0,10.184313085686012,81.46111111111111,0.35103308047523163,2.9835664214770805,1.8946381214437182,1.031766448201236,81.35,1.9039099790928413,0.45789149784833727,0.3038826072442205,1.3821296153914528
CERT,32.0,47.0,10.182068968460484,80.6888888888889,0.3528301781568454,2.967879952384141,1.856597422965205,1.0189349830455394,81.4,1.9162730309051326,0.4256119405587679,0.3038826072442205,1.396610600884495
mpnet-base-paddle,33.0,31.0,10.172777511184599,82.91111111111111,0.36241469912545243,3.0086647720257838,1.907318354269889,1.056262881680293,81.1,1.922454556811278,0.49256213345565275,0.3038826072442205,1.401437596048842
1,34.0,38.0,10.170971320422115,81.97777777777777,0.4001537504393425,3.027488534937311,1.8819578886175472,1.0626786142581413,81.25,1.951301677706624,0.49495321177339857,0.2908123875778024,1.1793958184888649
BERT + BAM,35.0,34.0,10.163569985935641,82.29444444444445,0.3684050247308318,2.986703715295669,1.8978081796502608,1.029433454536564,81.1,1.9183335395405143,0.4806067418669233,0.3038826072442205,1.3869566105558
WT-VAT-BERT (Base),36.0,59.0,10.16333770607746,79.45555555555555,0.33545823390124524,2.9616053647469656,1.846030562276729,1.0119360020515231,81.35,1.8833048927390228,0.42082978392327614,0.2908123875778024,1.3700621274805846
elasticbert-large-12L,37.0,55.0,10.143475150145806,79.93888888888888,0.3414485595066246,2.9145459574681474,1.8534273647586623,1.0399319260275883,81.15,1.9018494704574593,0.42919855803538665,0.2908123875778024,1.3700621274805846
segaBERT-large,38.0,43.0,10.1407260353113,81.43333333333334,0.37499438289674913,2.974154540021317,1.8576541090340528,1.0282669577042278,80.95,1.936878117258951,0.4280030188765137,0.3038826072442205,1.4127005847656526
LV-BERT-base,39.0,36.0,10.138296415729975,82.1277777777778,0.38338083874428025,2.971017246202729,1.8893546910994803,1.0393486776114202,80.9,1.9080309963636048,0.4602825761660832,0.3038826072442205,1.3893701081379737
Span-Extractive BERT on STILTs,40.0,35.0,10.126783645945835,82.27222222222223,0.37858857825997677,2.9647426585655534,1.8830145746863944,1.0416816712760926,80.80000000000001,1.905970487728223,0.4770201243903044,0.3038826072442205,1.3861521113617423
AMBERT-BASE,41.0,45.0,10.126103588947892,81.03333333333335,0.35941953632276274,2.986703715295669,1.8777311443421565,1.0177684862132035,80.85,1.9080309963636048,0.43398071467087845,0.3038826072442205,1.3974151000785526
SpanBERT (single-task training),42.0,32.0,10.12513240375615,82.77777777777777,0.38517793642589404,2.974154540021317,1.8893546910994803,1.0440146649407644,80.7,1.9430596431650966,0.47223796775481264,0.3038826072442205,1.4143095831537684
u-PMLM-R (Huawei Noah's Ark Lab),43.0,44.0,10.119414746342917,81.32777777777778,0.34084952694608667,2.95533077710979,1.8851279468240898,1.0428481681084285,80.80000000000001,1.8977284531866956,0.4692491198576303,0.3038826072442205,1.3797161178092792
GAT-bert-base,44.0,58.0,10.11414630286978,79.56666666666666,0.3402504943855487,2.9490561894726137,1.846030562276729,1.0189349830455394,80.9,1.89154692728055,0.42142755350271255,0.2908123875778024,1.3692576282865265
BERT on STILTs,45.0,37.0,10.106396012945973,81.98333333333333,0.37199922009405945,2.9584680709283773,1.8682209697225285,1.032349696617404,80.65,1.910091504998987,0.4788134331286138,0.3038826072442205,1.3837386137795686
HF bert-large-uncased (default fine-tuning),46.0,52.0,10.103670245115508,80.24444444444445,0.3684050247308318,2.967879952384141,1.8428605040701864,0.9996877853119946,80.75,1.9039099790928413,0.41186324023172904,0.3038826072442205,1.3845431129736265
KerasNLP XLM-R,47.5,49.5,10.10289723115779,80.40555555555555,0.33725533158285903,3.0149393596629595,1.8608241672405954,1.0271004608718919,80.65,1.9121520136343686,0.41365654897003845,0.3038826072442205,1.4062645912131895
KerasNLP RoBERTa,47.5,49.5,10.10289723115779,80.40555555555555,0.33725533158285903,3.0149393596629595,1.8608241672405954,1.0271004608718919,80.65,1.9121520136343686,0.41365654897003845,0.3038826072442205,1.4062645912131895
BERT: 24-layers  16-heads  1024-hidden,49.0,48.0,10.102527642684745,80.52222222222223,0.36241469912545243,2.977291833839905,1.846030562276729,1.0154354925485314,80.69999999999999,1.910091504998987,0.4190364751849666,0.3038826072442205,1.388565608943916
BERT-EMD(6-layer; Single model; No DA),50.0,62.0,10.077248229166822,78.70555555555556,0.2845404662555205,2.9270951327424988,1.8618808533094429,1.0171852377970352,80.65,1.8688813322913498,0.42860078845595023,0.3038826072442205,1.3531676444053686
roberta-large-12L,51.0,56.0,10.069593165092707,79.81666666666666,0.3558253409595351,2.967879952384141,1.8481439344144241,1.0434314165245964,80.45,1.8874259100097863,0.40229892696074543,0.2908123875778024,1.3805206170033373
BERT + Single-task Adapters,52.0,53.0,10.067686359044082,80.16111111111111,0.35462727583845927,2.9584680709283773,1.8280668991063203,1.011352753635355,80.45,1.9039099790928413,0.4280030188765137,0.3038826072442205,1.3708666266746423
MULTIPLE_ADAPTER_T5_BASE,53.0,51.0,10.063149050812356,80.32777777777777,0.3240766152510244,2.942781601835438,1.8692776557913757,1.0236009703748836,80.35,1.9265755740820418,0.45908703700721026,0.2908123875778024,1.3821296153914528
s0,54.0,71.0,10.052272281358876,77.76111111111112,0.2803472383317549,2.9145459574681474,1.8354637015882533,1.013102498883859,80.5,1.8709418409267313,0.4238186318204585,0.28147651638750376,1.3507541468231952
Bigs-128-1000k,55.0,41.0,10.049354369037347,81.49444444444444,0.38577696898643205,2.977291833839905,1.8270102130374728,1.0224344735425477,80.2,1.8874259100097863,0.463869193642702,0.3038826072442205,1.3764981210330476
KI-BERT,56.0,54.0,10.0344940315477,80.03333333333333,0.3330621036590935,2.9647426585655534,1.8185567244866923,0.9996877853119945,80.2,1.879183875468259,0.41425431854947486,0.3421596791244449,1.3587991387637741
BERT-of-Theseus (6-layer; single model),57.0,74.0,10.032526423651388,77.09444444444445,0.2863375639371343,2.892584900738032,1.8048198055916735,0.9897725622371382,80.44999999999999,1.846215737302149,0.39572346158694427,0.3038826072442205,1.323401174225227
TinyBERT (6-layer; Single model),58.0,67.0,10.032349628827976,78.13888888888889,0.30610563843488625,2.9208205451053226,1.7953096309720449,0.9839400780754579,80.35,1.8626998063852043,0.4184387056055302,0.3038826072442205,1.3499496476291373
Bert-n-Pals,59.0,60.0,10.028442263441526,79.07777777777777,0.3126949966008036,2.9302324265610866,1.8502573065521195,1.006103517889843,80.2,1.8668208236559676,0.4507182628950997,0.2908123875778024,1.3483406492410215
DeepPavlov Multitask PalBert,60.0,61.0,10.026839713865987,78.80555555555556,0.2881346616187481,2.9302324265610866,1.843917190139034,1.013102498883859,80.2,1.8709418409267313,0.4584892674277738,0.2908123875778024,1.3459271516588478
Anonymous,61.0,77.0,10.019584470660918,74.75,0.3150911268429553,2.9302324265610866,1.8048198055916735,0.701647844650134,80.55,1.8523972632082948,0.3885502266337066,0.2908123875778024,1.3427091548826162
SqueezeBERT (4.3x faster than BERT-base on smartphone),62.0,68.0,10.019044072473928,78.05555555555556,0.2785501406501411,2.867486550189329,1.8544840508275098,1.0107695052191872,80.25,1.8565182804790583,0.4375673321474973,0.3038826072442205,1.3121381855084164
Hanxiong Huang,63.0,75.0,10.014940454299579,75.85555555555555,0.29532305234520334,2.9270951327424988,1.7857994563524169,0.962359886677241,80.3,1.8750628581974953,0.3831703004187783,0.2492677607809735,1.3563856411816002
Routed BERTs,64.0,46.0,10.009261095137468,80.73333333333333,0.3360572664617832,2.9365070141982623,1.831236957312863,1.0241842187910517,79.9,1.9080309963636048,0.4782156635491774,0.3038826072442205,1.365235132316237
KerasNLP 12/05/2022 Trial 2,65.0,78.0,10.006229991536788,74.63888888888889,0.3126949966008036,2.9333697203796745,1.8005930613162828,0.9775243454976097,80.3,1.8400342113960035,0.36882383051230305,0.20445557906754006,1.3185741790608794
Macaron Net-base,66.0,57.0,10.001479824452465,79.66111111111111,0.34504275486985225,2.9490561894726137,1.8259535269686251,1.0136857473000274,79.9,1.8874259100097863,0.42142755350271255,0.3038826072442205,1.3668441307043528
ReptileDistil,67.0,64.0,9.999752567660824,78.52777777777777,0.2869365964976722,2.911408663649559,1.8449738762078818,1.009019759970683,80.0,1.8626998063852043,0.43936064088580673,0.3038826072442205,1.3394911581063846
SesameBERT-Base,68.0,63.0,9.982070751856682,78.54444444444444,0.31569015940349326,2.95533077710979,1.8354637015882533,1.0031872758090028,79.8,1.8750628581974953,0.40409223569905484,0.3038826072442205,1.3459271516588478
Pavan Neerudu - BERT,69.0,73.0,9.960249155908858,77.5611111111111,0.3360572664617832,2.9333697203796745,1.8048198055916735,0.98627307174013,79.69999999999999,1.8709418409267313,0.3825725308393419,0.28147651638750376,1.3467316508529057
StackingBERT-Base,70.0,66.0,9.948966575115831,78.39444444444445,0.33665629902232114,2.9459188956540263,1.8185567244866923,0.9722751097520974,79.55000000000001,1.8565182804790583,0.400505618222436,0.3038826072442205,1.3563856411816004
KRISFU,71.0,70.0,9.937775512453955,77.77777777777777,0.3138930617218795,2.9019967821937955,1.836520387657101,0.9676091224227533,79.5,1.8730023495621135,0.3939301528486349,0.3038826072442205,1.3491451484350792
MobileBERT,72.0,65.0,9.933457212557759,78.46666666666667,0.30610563843488625,2.9051340760123834,1.831236957312863,0.9973547916473225,79.4,1.8874259100097863,0.42082978392327614,0.3038826072442205,1.3491451484350792
Pocket GLUE,73.0,72.0,9.924189203548973,77.57222222222224,0.29532305234520334,2.8988594883752077,1.8344070155194057,0.9851065749077941,79.4,1.8565182804790583,0.401701157381309,0.3038826072442205,1.3419046556885583
CAMTL,74.0,69.0,9.910780476468029,77.9,0.31748725708510706,2.9051340760123834,1.8248968408997774,1.006103517889843,79.25,1.8647603150205858,0.4351762538297514,0.27167385163769026,1.3217921758371112
EL-BERT(6-Layer  Single model),75.0,76.0,9.857456524216198,75.6,0.2857385313765964,2.8549373749149773,1.8048198055916735,0.9413629436951922,79.0,1.8585787891144403,0.35806397808244655,0.3038826072442205,1.309724687926243
WARP with RoBERTa,76.0,40.0,9.833831851629506,81.61666666666666,0.32287855012994854,3.021213947300135,1.8185567244866923,1.0399319260275883,78.15,1.9265755740820418,0.5039197554649456,0.3038826072442205,1.4175275799299998
distilbert-base-uncased,77.0,80.0,9.829954642535906,73.64444444444445,0.2743569127263755,2.89572219455662,1.8037631195228256,0.8282127509585954,78.9,1.8297316682190943,0.3233933424751312,0.3038826072442205,1.3105291871203004
1111,78.0,85.0,9.699693177833737,71.36666666666666,0.21445365667258176,2.8267017305476863,1.6790741633988109,0.9349472111173439,78.0,1.7864609868760752,0.3467063560731536,0.26513874180448116,1.2437557540134958
Bag-of-words only BoW-BERT (Base),79.0,86.0,9.671781316197151,70.05,0.08566165615692513,2.720033740715698,1.6706206748480306,0.9454456826083683,77.9,1.7761584436991658,0.3610528259796289,0.3038826072442205,1.2831762145223324
Bort (Alexa AI),80.0,25.0,9.611051471118813,83.57222222222224,0.3827818061837423,3.0180766534815473,1.9696628323318963,1.0352659386982441,75.95,1.9018494704574593,0.4943554421939621,0.3323570143746314,1.415114082347826
BiLSTM+ELMo+Attn,81.0,87.0,9.200405137863989,70.02777777777777,0.20127494034074714,2.8361136120034502,1.7160581758084765,0.8544589296861564,73.7,1.6442858910347264,0.3520862822880818,0.3038826072442205,1.1954858023700226
RefBERT,82.0,81.0,9.176436325830991,73.13888888888889,0.2869365964976722,2.9145459574681474,1.783686084214722,0.8824548536622218,73.0,1.7988240386883663,0.36882383051230305,0.25580287061418255,1.2968527008213164
RefBERT,83.0,82.0,9.15421410360877,73.11666666666667,0.2869365964976722,2.9145459574681474,1.783686084214722,0.8824548536622218,72.8,1.7988240386883663,0.36882383051230305,0.25580287061418255,1.2968527008213164
RefBERT,84.0,83.0,9.135382128384059,71.81666666666666,0.21744881947527145,2.9145459574681474,1.783686084214722,0.8824548536622218,72.7,1.7988240386883663,0.36882383051230305,0.25580287061418255,1.2968527008213164
RefBERT,85.0,84.0,9.107604350606278,71.78888888888889,0.21744881947527145,2.9145459574681474,1.783686084214722,0.8824548536622218,72.44999999999999,1.7988240386883663,0.36882383051230305,0.25580287061418255,1.2968527008213164
GLUE Human Baselines,86.0,23.0,8.959508716332712,87.05,0.3977576201971908,3.0682733545789533,1.7657224210443128,1.0807593151593502,69.95,1.879183875468259,0.5595123263525374,0.4476550235748195,1.4867145106189783
ZHIYUAN,87.0,79.0,3.807202070481852,74.06666666666666,0.3414485595066246,2.986703715295669,1.8999215517879562,1.0609288690096372,23.95,1.905970487728223,0.4883777463995974,0.22359411500765228,1.4078735896013053
