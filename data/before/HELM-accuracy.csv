Model,Rank,WinRate,MMLU - EM,BoolQ - EM,NarrativeQA - F1,NaturalQuestions (closed-book) - F1,NaturalQuestions (open-book) - F1,QuAC - F1,HellaSwag - EM,OpenbookQA - EM,TruthfulQA - EM,MS MARCO (regular) - RR@10,MS MARCO (TREC) - NDCG@10,CNN/DailyMail - ROUGE-2,XSUM - ROUGE-2,IMDB - EM,CivilComments - EM,RAFT - EM
Llama 2 (70B),1.0,0.682051282051282,0.582,0.886,0.77,0.458,0.674,0.484,,,0.554,,,,,0.961,0.652,0.727
text-davinci-003,2.0,0.6066808191808192,0.569,0.881,0.727,0.406,0.77,0.525,0.822,0.646,0.593,0.368,0.644,0.156,0.124,0.848,0.684,0.759
text-davinci-002,3.0,0.56505994005994,0.568,0.877,0.727,0.383,0.713,0.445,0.815,0.594,0.61,0.421,0.664,0.153,0.144,0.948,0.668,0.733
LLaMA (65B),4.0,0.5581196581196581,0.584,0.871,0.755,0.431,0.672,0.401,,,0.508,,,,,0.962,0.655,0.702
Palmyra X (43B),5.0,0.5439005439005439,0.609,0.896,0.742,0.413,,0.473,,,0.616,,,0.049,0.149,0.935,0.008,0.701
gpt-3.5-turbo-0301,6.0,0.4871794871794871,0.59,0.74,0.663,0.39,0.624,0.512,,,0.609,,,,,0.899,0.674,0.768
Mistral v0.1 (7B),7.0,0.4256410256410256,0.572,0.874,0.716,0.365,0.687,0.423,,,0.422,,,,,0.962,0.624,0.707
gpt-3.5-turbo-0613,8.0,0.4034188034188035,0.391,0.87,0.625,0.348,0.675,0.485,,,0.339,,,,,0.943,0.696,0.748
Cohere Command beta (52.4B),9.0,0.3927947052947053,0.452,0.856,0.752,0.372,0.76,0.432,0.811,0.582,0.269,0.472,0.762,0.161,0.152,0.96,0.601,0.667
Jurassic-2 Jumbo (178B),10.0,0.35872877122877117,0.48,0.829,0.733,0.385,0.669,0.435,0.788,0.558,0.437,0.398,0.661,0.149,0.182,0.938,0.57,0.746
LLaMA (30B),11.0,0.35641025641025637,0.531,0.861,0.752,0.408,0.666,0.39,,,0.344,,,,,0.927,0.549,0.752
Llama 2 (13B),12.0,0.30341880341880334,0.507,0.811,0.744,0.376,0.637,0.424,,,0.33,,,,,0.962,0.588,0.707
Anthropic-LM v4-s3 (52B),13.0,0.2678321678321678,0.481,0.815,0.728,0.288,0.686,0.431,0.807,0.558,0.368,,,0.154,0.134,0.934,0.61,0.699
