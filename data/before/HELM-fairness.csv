Model,Rank,WinRate,MMLU - EM (Fairness),BoolQ - EM (Fairness),NarrativeQA - F1 (Fairness),NaturalQuestions (closed-book) - F1 (Fairness),NaturalQuestions (open-book) - F1 (Fairness),QuAC - F1 (Fairness),HellaSwag - EM (Fairness),OpenbookQA - EM (Fairness),TruthfulQA - EM (Fairness),MS MARCO (regular) - RR@10 (Fairness),MS MARCO (TREC) - NDCG@10 (Fairness),IMDB - EM (Fairness),CivilComments - EM (Fairness),RAFT - EM (Fairness)
Llama 2 (70B),1.0,0.7743589743589745,0.557,0.859,0.709,0.4,0.637,0.414,,,0.434,,,0.954,0.551,0.7
text-davinci-003,2.0,0.684920634920635,0.537,0.858,0.664,0.356,0.721,0.45,0.729,0.578,0.491,0.335,0.633,0.833,0.559,0.705
LLaMA (65B),3.0,0.6264957264957266,0.551,0.847,0.661,0.375,0.633,0.333,,,0.42,,,0.953,0.574,0.668
Palmyra X (43B),4.0,0.5811965811965811,0.588,0.875,0.651,0.362,,0.399,,,0.542,,,0.918,0.006,0.672
text-davinci-002,5.0,0.4846764346764346,0.531,0.837,0.646,0.32,0.659,0.353,0.703,0.54,0.515,0.373,0.639,0.934,0.463,0.671
Mistral v0.1 (7B),6.0,0.4247863247863248,0.542,0.842,0.644,0.3,0.625,0.353,,,0.332,,,0.952,0.52,0.664
Jurassic-2 Jumbo (178B),7.0,0.40964590964590963,0.45,0.792,0.658,0.327,0.62,0.34,0.655,0.488,0.354,0.342,0.62,0.933,0.507,0.711
LLaMA (30B),8.0,0.40256410256410263,0.496,0.813,0.657,0.356,0.621,0.325,,,0.266,,,0.913,0.508,0.718
Cohere Command beta (52.4B),9.0,0.3923076923076923,0.407,0.822,0.657,0.296,0.706,0.316,0.699,0.508,0.222,0.45,0.748,0.957,0.544,0.627
Llama 2 (13B),10.0,0.36495726495726494,0.466,0.732,0.657,0.309,0.58,0.351,,,0.274,,,0.957,0.489,0.673
Anthropic-LM v4-s3 (52B),11.0,0.32222222222222224,0.447,0.782,0.646,0.239,0.642,0.356,0.695,0.482,0.3,,,0.925,0.512,0.67
MPT (30B),12.0,0.3188034188034188,0.41,0.631,0.653,0.287,0.624,0.318,,,0.19,,,0.955,0.553,0.68
TNLG v2 (530B),13.0,0.16050061050061049,0.418,0.767,0.632,0.318,0.598,0.313,0.678,0.504,0.197,0.341,0.612,0.936,0.48,0.644
