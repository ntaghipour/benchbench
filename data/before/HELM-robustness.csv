Model,Rank,WinRate,MMLU - EM (Robustness),BoolQ - EM (Robustness),NarrativeQA - F1 (Robustness),NaturalQuestions (closed-book) - F1 (Robustness),NaturalQuestions (open-book) - F1 (Robustness),QuAC - F1 (Robustness),HellaSwag - EM (Robustness),OpenbookQA - EM (Robustness),TruthfulQA - EM (Robustness),MS MARCO (regular) - RR@10 (Robustness),MS MARCO (TREC) - NDCG@10 (Robustness),IMDB - EM (Robustness),CivilComments - EM (Robustness),RAFT - EM (Robustness)
Llama 2 (70B),1.0,0.764957264957265,0.545,0.863,0.722,0.42,0.639,0.362,,,0.468,,,0.949,0.59,0.673
text-davinci-003,2.0,0.7044566544566545,0.517,0.858,0.694,0.369,0.73,0.42,0.798,0.572,0.516,0.304,0.616,0.779,0.594,0.714
Palmyra X (43B),3.0,0.6581196581196581,0.566,0.878,0.672,0.363,,0.383,,,0.568,,,0.904,0.006,0.677
text-davinci-002,4.0,0.5548229548229549,0.525,0.841,0.638,0.299,0.665,0.319,0.776,0.52,0.547,0.344,0.628,0.925,0.567,0.666
gpt-3.5-turbo-0301,5.0,0.4871794871794871,0.525,0.66,0.602,0.327,0.556,0.411,,,0.566,,,0.857,0.605,0.705
LLaMA (65B),6.0,0.4794871794871795,0.504,0.84,0.567,0.388,0.624,0.275,,,0.448,,,0.935,0.566,0.655
Mistral v0.1 (7B),7.0,0.47094017094017093,0.533,0.837,0.649,0.305,0.631,0.31,,,0.339,,,0.954,0.521,0.652
Llama 2 (13B),8.0,0.3333333333333333,0.444,0.753,0.682,0.324,0.563,0.294,,,0.287,,,0.954,0.47,0.652
LLaMA (30B),9.0,0.3162393162393162,0.461,0.791,0.611,0.36,0.612,0.273,,,0.281,,,0.893,0.503,0.67
Anthropic-LM v4-s3 (52B),10.0,0.31196581196581197,0.434,0.756,0.663,0.245,0.632,0.313,0.766,0.472,0.326,,,0.928,0.514,0.6
Jurassic-2 Jumbo (178B),11.0,0.3108669108669109,0.417,0.729,0.66,0.315,0.599,0.314,0.754,0.47,0.39,0.337,0.607,0.896,0.449,0.69
Cohere Command beta (52.4B),12.0,0.3059218559218559,0.387,0.811,0.57,0.289,0.679,0.238,0.774,0.492,0.229,0.434,0.734,0.933,0.535,0.599
Falcon-Instruct (40B),13.0,0.2786324786324786,0.446,0.781,0.508,0.335,0.591,0.212,,,0.338,,,0.938,0.523,0.523
