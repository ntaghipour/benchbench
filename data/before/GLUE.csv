Model,Rank,Average,CoLA,SST-2,MRPC,STS-B,QQP,QNLI,RTE,WNLI,MNLI
Turing ULR v6,1.0,91.27777777777777,73.3,97.5,93.25,93.3,83.65,96.7,93.6,97.9,92.3
Vega v1,2.0,91.27222222222223,73.8,97.9,93.55,93.3,83.9,96.7,92.4,97.9,92.0
Turing NLR v5 ,3.0,91.17777777777778,72.6,97.6,92.75,93.5,83.75,97.9,94.1,95.9,92.5
DeBERTa + CLEVER,4.0,91.10555555555555,74.7,97.6,92.19999999999999,93.25,83.75,96.7,93.2,96.6,91.94999999999999
ERNIE,5.0,91.08888888888889,75.5,97.8,92.85,92.8,83.05000000000001,97.3,92.6,95.9,92.0
StructBERT + CLEVER,6.0,91.01111111111112,75.3,97.7,92.9,93.3,83.19999999999999,97.4,92.5,95.2,91.6
DeBERTa / TuringNLRv4,7.0,90.76666666666668,71.5,97.5,93.0,92.75,83.5,99.2,93.2,94.5,91.75
MacALBERT + DKM,8.0,90.6888888888889,74.8,97.0,93.55,92.69999999999999,82.65,97.8,92.0,94.5,91.19999999999999
ALBERT + DAAF + NAS,9.0,90.56666666666668,73.5,97.2,93.0,92.7,83.55,97.5,91.7,94.5,91.44999999999999
T5,10.0,90.30555555555556,71.6,97.5,91.6,92.94999999999999,82.85,96.9,92.8,94.5,92.05000000000001
MT-DNN-SMART,11.0,89.85555555555555,69.5,97.5,92.65,92.7,82.05000000000001,99.2,89.7,94.5,90.9
NEZHA-Large,12.0,89.84999999999998,71.7,97.3,92.15,92.15,82.95,96.2,90.3,94.5,91.4
ANNA,13.0,89.8111111111111,68.7,97.0,91.4,92.9,82.9,96.0,91.8,95.9,91.69999999999999
Funnel-Transformer (Ensemble B10-10-10H1024),14.0,89.70555555555556,70.5,97.5,92.30000000000001,92.44999999999999,83.05000000000001,95.8,90.0,94.5,91.25
ELECTRA-Large + Standard Tricks,15.0,89.44999999999999,71.7,97.1,91.9,92.7,83.19999999999999,95.8,89.8,91.8,91.05
2digit LANet,16.0,89.28333333333333,71.8,97.3,91.0,92.85,83.0,96.4,91.1,88.4,91.69999999999999
 DropAttack-RoBERTa-large,17.0,88.77777777777777,70.3,96.7,91.35,91.94999999999999,82.8,95.3,89.9,89.7,91.0
FreeLB-RoBERTa (ensemble),18.0,88.41111111111111,68.0,96.8,91.94999999999999,92.19999999999999,82.55,95.6,88.7,89.0,90.9
HIRE-RoBERTa,19.0,88.32777777777778,68.6,97.1,91.85,92.2,82.25,95.5,87.9,89.0,90.55000000000001
ELECTRA-large-M (bert4keras),20.0,88.27222222222221,69.3,95.8,90.9,91.15,82.8,93.8,87.9,91.8,91.0
RoBERTa,21.0,88.10555555555555,67.8,96.7,91.05,92.05000000000001,82.25,95.4,88.2,89.0,90.5
MT-DNN-ensemble,22.0,87.56111111111109,68.4,96.5,91.5,90.9,81.80000000000001,96.0,86.3,89.0,87.65
GLUE Human Baselines,23.0,87.05,66.4,97.8,83.55,92.65,69.95,91.2,93.6,95.9,92.4
ELECTRA-Large-NewSCL(single),24.0,85.56666666666665,73.3,97.2,91.45,91.85,82.94999999999999,95.6,86.9,60.3,90.55
Bort (Alexa AI),25.0,83.57222222222224,63.9,96.2,93.19999999999999,88.75,75.95,92.3,82.7,71.2,87.94999999999999
ConvBERT base,26.0,83.21666666666667,67.8,95.7,89.85,90.05000000000001,81.5,93.2,77.9,65.1,87.85
Snorkel MeTaL,27.0,83.18888888888888,63.8,96.2,90.0,89.9,81.5,93.9,80.9,65.1,87.4
XLM (English only),28.0,83.1222222222222,62.9,95.6,88.9,88.5,81.5,94.0,76.0,71.9,88.8
ConvBERT-base-paddle-v1.1,29.0,83.10000000000001,66.3,95.4,90.1,89.6,81.95,93.3,78.2,65.1,87.95
SemBERT,30.0,82.92777777777776,62.3,94.6,89.75,87.25,81.3,94.6,84.5,65.1,86.94999999999999
mpnet-base-paddle,31.0,82.91111111111111,60.5,95.9,90.25,90.55,81.1,93.3,82.4,65.1,87.1
SpanBERT (single-task training),32.0,82.77777777777777,64.3,94.8,89.4,89.5,80.7,94.3,79.0,65.1,87.9
distilRoBERTa+GAL (6-layer transformer single model),33.0,82.65000000000002,60.0,95.3,90.55000000000001,89.8,81.65,92.7,81.8,65.1,86.95
BERT + BAM,34.0,82.29444444444445,61.5,95.2,89.8,88.25,81.1,93.1,80.4,65.1,86.19999999999999
Span-Extractive BERT on STILTs,35.0,82.27222222222223,63.2,94.5,89.1,89.30000000000001,80.80000000000001,92.5,79.8,65.1,86.15
LV-BERT-base,36.0,82.1277777777778,64.0,94.7,89.4,89.1,80.9,92.6,77.0,65.1,86.35
BERT on STILTs,37.0,81.98333333333333,62.1,94.3,88.4,88.5,80.65,92.7,80.1,65.1,86.0
1,38.0,81.97777777777777,66.8,96.5,89.05000000000001,91.1,81.25,94.7,82.8,62.3,73.3
RobustRoBERTa,39.0,81.9,63.6,96.8,90.1,89.94999999999999,81.45,95.1,50.3,80.1,89.7
WARP with RoBERTa,40.0,81.61666666666666,53.9,96.3,86.05000000000001,89.15,78.15,93.5,84.3,65.1,88.1
Bigs-128-1000k,41.0,81.49444444444444,64.4,94.9,86.45,87.65,80.2,91.6,77.6,65.1,85.55
CombinedKD-TinyRoBERTa (6 layer 82M parameters  MATE-KD + AnnealingKD),42.0,81.46111111111111,58.6,95.1,89.65,88.45,81.35,92.4,76.6,65.1,85.9
segaBERT-large,43.0,81.43333333333334,62.6,94.8,87.9,88.15,80.95,94.0,71.6,65.1,87.80000000000001
u-PMLM-R (Huawei Noah's Ark Lab),44.0,81.32777777777778,56.9,94.2,89.2,89.4,80.80000000000001,92.1,78.5,65.1,85.75
AMBERT-BASE,45.0,81.03333333333335,60.0,95.2,88.85,87.25,80.85,92.6,72.6,65.1,86.85
Routed BERTs,46.0,80.73333333333333,56.1,93.6,86.65,87.8,79.9,92.6,80.0,65.1,84.85
CERT,47.0,80.6888888888889,58.9,94.6,87.85,87.35,81.4,93.0,71.2,65.1,86.80000000000001
BERT: 24-layers  16-heads  1024-hidden,48.0,80.52222222222223,60.5,94.9,87.35,87.05,80.69999999999999,92.7,70.1,65.1,86.30000000000001
KerasNLP XLM-R,49.5,80.40555555555555,56.3,96.1,88.05,88.05000000000001,80.65,92.8,69.2,65.1,87.4
KerasNLP RoBERTa,49.5,80.40555555555555,56.3,96.1,88.05,88.05000000000001,80.65,92.8,69.2,65.1,87.4
MULTIPLE_ADAPTER_T5_BASE,51.0,80.32777777777777,54.1,93.8,88.44999999999999,87.75,80.35,93.5,76.8,62.3,85.9
HF bert-large-uncased (default fine-tuning),52.0,80.24444444444445,61.5,94.6,87.2,85.7,80.75,92.4,68.9,65.1,86.05000000000001
BERT + Single-task Adapters,53.0,80.16111111111111,59.2,94.3,86.5,86.69999999999999,80.45,92.4,71.6,65.1,85.2
KI-BERT,54.0,80.03333333333333,55.6,94.5,86.05000000000001,85.69999999999999,80.2,91.2,69.3,73.3,84.45
elasticbert-large-12L,55.0,79.93888888888888,57.0,92.9,87.7,89.15,81.15,92.3,71.8,62.3,85.15
roberta-large-12L,56.0,79.81666666666666,59.4,94.6,87.44999999999999,89.44999999999999,80.45,91.6,67.3,62.3,85.80000000000001
Macaron Net-base,57.0,79.66111111111111,57.6,94.0,86.4,86.9,79.9,91.6,70.5,65.1,84.95
GAT-bert-base,58.0,79.56666666666666,56.8,94.0,87.35,87.35,80.9,91.8,70.5,62.3,85.1
WT-VAT-BERT (Base),59.0,79.45555555555555,56.0,94.4,87.35,86.75,81.35,91.4,70.4,62.3,85.15
Bert-n-Pals,60.0,79.07777777777777,52.2,93.4,87.55,86.25,80.2,90.6,75.4,62.3,83.8
DeepPavlov Multitask PalBert,61.0,78.80555555555556,48.1,93.4,87.25,86.85,80.2,90.8,76.7,62.3,83.65
BERT-EMD(6-layer; Single model; No DA),62.0,78.70555555555556,47.5,93.3,88.1,87.19999999999999,80.65,90.7,71.7,65.1,84.1
SesameBERT-Base,63.0,78.54444444444444,52.7,94.2,86.85,86.0,79.8,91.0,67.6,65.1,83.65
ReptileDistil,64.0,78.52777777777777,47.9,92.8,87.30000000000001,86.5,80.0,90.4,73.5,65.1,83.25
MobileBERT,65.0,78.46666666666667,51.1,92.6,86.65,85.5,79.4,91.6,70.4,65.1,83.85
StackingBERT-Base,66.0,78.39444444444445,56.2,93.9,86.05000000000001,83.35,79.55000000000001,90.1,67.0,65.1,84.30000000000001
TinyBERT (6-layer; Single model),67.0,78.13888888888889,51.1,93.1,84.94999999999999,84.35,80.35,90.4,70.0,65.1,83.9
SqueezeBERT (4.3x faster than BERT-base on smartphone),68.0,78.05555555555556,46.5,91.4,87.75,86.65,80.25,90.1,73.2,65.1,81.55
CAMTL,69.0,77.9,53.0,92.6,86.35,86.25,79.25,90.5,72.8,58.2,82.15
KRISFU,70.0,77.77777777777777,52.4,92.5,86.9,82.95,79.5,90.9,65.9,65.1,83.85
s0,71.0,77.76111111111112,46.8,92.9,86.85,86.85,80.5,90.8,70.9,60.3,83.95
Pocket GLUE,72.0,77.57222222222224,49.3,92.4,86.8,84.45,79.4,90.1,67.2,65.1,83.4
Pavan Neerudu - BERT,73.0,77.5611111111111,56.1,93.5,85.4,84.55,79.69999999999999,90.8,64.0,60.3,83.7
BERT-of-Theseus (6-layer; single model),74.0,77.09444444444445,47.8,92.2,85.4,84.85,80.44999999999999,89.6,66.2,65.1,82.25
Hanxiong Huang,75.0,75.85555555555555,49.3,93.3,84.5,82.5,80.3,91.0,64.1,53.4,84.3
EL-BERT(6-Layer  Single model),76.0,75.6,47.7,91.0,85.4,80.7,79.0,90.2,59.9,65.1,81.4
Anonymous,77.0,74.75,52.6,93.4,85.4,60.150000000000006,80.55,89.9,65.0,62.3,83.45
KerasNLP 12/05/2022 Trial 2,78.0,74.63888888888889,52.2,93.5,85.19999999999999,83.8,80.3,89.3,61.7,43.8,81.94999999999999
ZHIYUAN,79.0,74.06666666666666,57.0,95.2,89.9,90.94999999999999,23.95,92.5,81.7,47.9,87.5
distilbert-base-uncased,80.0,73.64444444444445,45.8,92.3,85.35,71.0,78.9,88.8,54.1,65.1,81.44999999999999
RefBERT,81.0,73.13888888888889,47.9,92.9,84.4,75.65,73.0,87.3,61.7,54.8,80.6
RefBERT,82.0,73.11666666666667,47.9,92.9,84.4,75.65,72.8,87.3,61.7,54.8,80.6
RefBERT,83.0,71.81666666666666,36.3,92.9,84.4,75.65,72.7,87.3,61.7,54.8,80.6
RefBERT,84.0,71.78888888888889,36.3,92.9,84.4,75.65,72.44999999999999,87.3,61.7,54.8,80.6
1111,85.0,71.36666666666666,35.8,90.1,79.45,80.15,78.0,86.7,58.0,56.8,77.3
Bag-of-words only BoW-BERT (Base),86.0,70.05,14.3,86.7,79.05000000000001,81.05,77.9,86.2,60.4,65.1,79.75
BiLSTM+ELMo+Attn,87.0,70.02777777777777,33.6,90.4,81.2,73.25,73.7,79.8,58.9,65.1,74.3
